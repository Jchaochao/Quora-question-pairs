{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.metrics import log_loss\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'input/'\n",
    "train = pd.read_csv(path+\"train.csv\").astype(str)\n",
    "test = pd.read_csv(path+\"test.csv\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_id                                          question1  \\\n",
       "0       0  How does the Surface Pro himself 4 compare wit...   \n",
       "1       1  Should I have a hair transplant at age 24? How...   \n",
       "2       2  What but is the best way to send money from Ch...   \n",
       "3       3                        Which food not emulsifiers?   \n",
       "4       4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  \n",
       "1        How much cost does hair transplant require?  \n",
       "2                      What you send money to China?  \n",
       "3                                  What foods fibre?  \n",
       "4                     How their can I start reading?  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Q1 = train['question1'].tolist()\n",
    "train_Q2 = train['question1'].tolist()\n",
    "test_Q1 = test['question1'].tolist()\n",
    "test_Q2 = test['question2'].tolist()\n",
    "labels = np.array(train['is_duplicate'].tolist())\n",
    "test_ids = np.array(test['test_id'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([int(x) for x in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "    Use re package separating punctuation from the  word, restore abbreviation \n",
    "    \n",
    "    Remove stop-words\n",
    "    \n",
    "    Correct spelling (optional)\n",
    "    \n",
    "    Lemmatize word (optional)\n",
    "    \n",
    "    Get word stem (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spell checker using word2vec**\n",
    "adapted from 'https://www.kaggle.com/cpmpml/spell-checker-using-word2vec', credit to CPMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KeyedVectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-aa1bced83604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mw_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mw_rank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KeyedVectors' is not defined"
     ]
    }
   ],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "words = word2vec.index2word\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "WORDS = w_rank\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    print(candidates(word))\n",
    "    return max(candidates(word), key=P)\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text, remove_stopwords=False, stem_words=False,spell_check=False,lemmer_words=False):\n",
    "\n",
    "    text = text.lower().split()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    #Credit to https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    if spell_check:\n",
    "        text = text.split()\n",
    "        #spell = SpellChecker()\n",
    "        #checked_words = [spell.correction(word) for word in text]\n",
    "        checked_words = [correction(word) for word in text]\n",
    "        text = \" \".join(checked_words)\n",
    "        \n",
    "    if lemmer_words:\n",
    "        text = text.split()\n",
    "        lemmer = WordNetLemmatizer()\n",
    "        lemmer_words = [lemmer.lemmatize(word) for word in text]\n",
    "        text = \" \".join(lemmer_words)\n",
    "\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'employ'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer('english').stem('employer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_Q1 = [clean(x) for x in train_Q1]\n",
    "clean_train_Q2 = [clean(x) for x in train_Q2]\n",
    "clean_test_Q1 = [clean(x) for x in test_Q1]\n",
    "clean_test_Q2 = [clean(x) for x in test_Q2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize our sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 109652 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=200000)\n",
    "tokenizer.fit_on_texts(clean_train_Q1 + clean_train_Q2 + clean_test_Q1 + clean_test_Q2)\n",
    "\n",
    "train_seq1 = tokenizer.texts_to_sequences(clean_train_Q1)\n",
    "train_seq2 = tokenizer.texts_to_sequences(clean_train_Q2)\n",
    "test_seq1 = tokenizer.texts_to_sequences(clean_test_Q1)\n",
    "test_seq2 = tokenizer.texts_to_sequences(clean_test_Q2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad the sequence into the same length where I set to 30 because almost all sentences are range from 3-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 30\n",
    "padded_train_seq1 = pad_sequences(train_seq1, maxlen=MAX_SEQ)\n",
    "padded_train_seq2 = pad_sequences(train_seq2, maxlen=MAX_SEQ)\n",
    "padded_test_seq1 = pad_sequences(test_seq1, maxlen=MAX_SEQ)\n",
    "padded_test_seq2 = pad_sequences(test_seq2, maxlen=MAX_SEQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract magic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide(x, y, val=0.0):\n",
    "    if y != 0.0:\n",
    "        val = float(x) / float(y)\n",
    "    return val\n",
    "\n",
    "def get_jaccard(seq1, seq2):\n",
    "    return 1 -  safe_divide(len(set(seq1) & set(seq2)),len(set(seq1) | set(seq2)))\n",
    "\n",
    "def get_len(seq):\n",
    "    return len(seq)\n",
    "\n",
    "def get_len_unique(seq1,seq2):\n",
    "    set1 = set(seq1)\n",
    "    set2 = set(seq2)\n",
    "    intersection = len(set(seq1) & set(seq2))\n",
    "    return len(set1) -intersection\n",
    "\n",
    "def get_features(seq1,seq2):\n",
    "\n",
    "    jaccard = np.array([ get_jaccard(x1,x2) for x1,x2 in zip(seq1,seq2)]).reshape(-1,1)\n",
    "    len1 = np.array([ get_len(x1)  for x1 in  seq1]).reshape(-1,1)\n",
    "    len2 = np.array([ get_len(x2)  for x2 in  seq2]).reshape(-1,1)\n",
    "\n",
    "    len1_unique = np.array([ get_len_unique(x1,x2)  for x1,x2 in  zip(seq1,seq2)]).reshape(-1,1)\n",
    "    len2_unique = np.array([ get_len_unique(x2,x1)  for x1,x2 in  zip(seq1,seq2)]).reshape(-1,1)\n",
    "\n",
    "    len_diff = np.abs(len2-len1)\n",
    "\n",
    "\n",
    "    features = np.hstack([jaccard,len1,len2,len1_unique,len2_unique,len_diff])\n",
    "    \n",
    "\n",
    "    return features\n",
    "    #return X_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_features(train['question1'].apply(lambda x:x.split(' ')),train['question2'].apply(lambda x:x.split(' ')))\n",
    "features_test = get_features(test['question1'].apply(lambda x:x.split(' ')),test['question2'].apply(lambda x:x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((features, features_test)))\n",
    "features = ss.transform(features)\n",
    "features_test = ss.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(embedding_file):\n",
    "    embeddings_index = {}\n",
    "    f = open(embedding_file)\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %d word vectors of glove.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "embeddings_index = embedding('glove.6B.300d.txt')\n",
    "#word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 37647\n"
     ]
    }
   ],
   "source": [
    "em_words = min(200000, len(word_index))+1\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((em_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    #if word in word2vec.vocab:\n",
    "    #    embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_rate = 0.1\n",
    "perm = np.random.permutation(len(padded_train_seq1))\n",
    "idx_train = perm[:int(len(padded_train_seq1)*(1-split_rate))]\n",
    "idx_val = perm[int(len(padded_train_seq1)*(1-split_rate)):]\n",
    "\n",
    "final_train_seq1 = np.vstack((padded_train_seq1[idx_train], padded_train_seq2[idx_train]))\n",
    "final_train_seq2 = np.vstack((padded_train_seq2[idx_train], padded_train_seq1[idx_train]))\n",
    "features_train = np.vstack((features[idx_train], features[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "final_val_seq1 = np.vstack((padded_train_seq1[idx_val], padded_train_seq2[idx_val]))\n",
    "final_val_seq2 = np.vstack((padded_train_seq2[idx_val], padded_train_seq1[idx_val]))\n",
    "features_val = np.vstack((features[idx_val], features[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "re_weight = True\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm = 200\n",
    "num_dense = 150\n",
    "act = 'relu'\n",
    "embedding_layer = Embedding(em_words,embedding_dim,weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ,trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=0.25, recurrent_dropout=0.25)\n",
    "\n",
    "Q1_input = Input(shape=(MAX_SEQ,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(Q1_input)\n",
    "encoding1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "Q2_input = Input(shape=(MAX_SEQ,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(Q2_input)\n",
    "encoding2 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "features_input = Input(shape=(features.shape[1],))\n",
    "features_dense = Dense(int(num_dense/2), activation=act)(features_input)\n",
    "\n",
    "merged = concatenate([encoding1, encoding2, features_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(0.25)(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(0.25)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 30, 300)      32895900    input_29[0][0]                   \n",
      "                                                                 input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_31 (InputLayer)           (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  (None, 200)          400800      embedding_15[0][0]               \n",
      "                                                                 embedding_15[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 75)           525         input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 475)          0           lstm_12[0][0]                    \n",
      "                                                                 lstm_12[1][0]                    \n",
      "                                                                 dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 475)          1900        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 475)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 150)          71400       dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 150)          600         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 150)          0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1)            151         dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 33,371,276\n",
      "Trainable params: 474,126\n",
      "Non-trainable params: 32,897,150\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "model = Model(inputs=[Q1_input, Q2_input, features_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = 'best_model' + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.309028344, 1: 0.472001959}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/5\n",
      "727722/727722 [==============================] - 1711s 2ms/step - loss: 0.3797 - acc: 0.7067 - val_loss: 0.3344 - val_acc: 0.6655\n",
      "Epoch 2/5\n",
      "727722/727722 [==============================] - 1575s 2ms/step - loss: 0.3185 - acc: 0.7335 - val_loss: 0.3065 - val_acc: 0.7350\n",
      "Epoch 3/5\n",
      "727722/727722 [==============================] - 3159s 4ms/step - loss: 0.3022 - acc: 0.7526 - val_loss: 0.2983 - val_acc: 0.7569\n",
      "Epoch 4/5\n",
      "727722/727722 [==============================] - 1419s 2ms/step - loss: 0.2894 - acc: 0.7669 - val_loss: 0.2931 - val_acc: 0.7574\n",
      "Epoch 5/5\n",
      "727722/727722 [==============================] - 1429s 2ms/step - loss: 0.2780 - acc: 0.7796 - val_loss: 0.2888 - val_acc: 0.7705\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([final_train_seq1 , final_train_seq2, features_train], labels_train, \\\n",
    "        validation_data=([final_val_seq1, final_val_seq2, features_val], labels_val, weight_val), \\\n",
    "        epochs=5, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345796/2345796 [==============================] - 1770s 755us/step\n",
      "2345796/2345796 [==============================] - 1854s 791us/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([padded_test_seq1, padded_test_seq2, features_test], batch_size=8192, verbose=1)\n",
    "preds += model.predict([padded_test_seq2, padded_test_seq1, features_test], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('submission/submission_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
